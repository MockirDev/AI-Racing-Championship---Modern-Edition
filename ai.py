import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
from collections import namedtuple, deque
import math

# Deneyimlerin saklanacaÄŸÄ± veri yapÄ±sÄ±
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ReplayMemory:
    """
    GeÃ§miÅŸ deneyimleri saklamak iÃ§in kullanÄ±lan hafÄ±za yapÄ±sÄ±.
    """
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        """Bir deneyimi hafÄ±zaya kaydeder."""
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        """HafÄ±zadan rastgele bir grup deneyim Ã¶rneÄŸi alÄ±r."""
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class DQN(nn.Module):
    """
    Derin Q-Network (DQN) sinir aÄŸÄ± modeli.
    """
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    def forward(self, x):
        """
        Ä°leri yayÄ±lÄ±m fonksiyonu. Durum (state) girdisini alÄ±r
        ve her eylem iÃ§in Q-deÄŸerlerini dÃ¶ndÃ¼rÃ¼r.
        """
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)

class Agent:
    """
    Yapay zeka ajanÄ±. Karar verme ve Ã¶ÄŸrenme sÃ¼reÃ§lerini yÃ¶netir.
    """
    def __init__(self, n_observations, n_actions, difficulty="medium", agent_id=0, behavior_type="balanced"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.agent_id = agent_id
        
        # Ä°ki sinir aÄŸÄ±: biri kararlar iÃ§in, diÄŸeri Ã¶ÄŸrenme hedeflerini sabitlemek iÃ§in
        self.policy_net = DQN(n_observations, n_actions).to(self.device)
        self.target_net = DQN(n_observations, n_actions).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval() # Target network'Ã¼ sadece deÄŸerlendirme modunda kullan

        # AI Difficulty System
        self.difficulty = difficulty.lower()
        difficulty_settings = self._get_difficulty_settings()
        
        # AI Behavioral Patterns System
        self.behavior_type = behavior_type
        self.behavioral_params = self._get_behavioral_params()
        
        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=difficulty_settings['learning_rate'], amsgrad=True)
        self.memory = ReplayMemory(difficulty_settings['memory_capacity'])
        
        self.n_actions = n_actions
        self.steps_done = 0
        
        # Epsilon-greedy stratejisi iÃ§in parametreler (zorluk seviyesine gÃ¶re ayarlandÄ±)
        self.eps_start = difficulty_settings['eps_start']
        self.eps_end = difficulty_settings['eps_end']
        self.eps_decay = difficulty_settings['eps_decay']
        self.skill_modifier = difficulty_settings['skill_modifier']
        
        # Multi-Agent Learning System
        self.shared_experiences = []  # DiÄŸer ajanlardan gelen deneyimler
        self.cooperation_factor = 0.1  # DiÄŸer ajanlardan Ã¶ÄŸrenme oranÄ±
        self.competition_mode = True
        self.rival_performance = {}  # Rakiplerin performans bilgileri
        
        # === AI STATISTICS & LEARNING TRACKING ===
        self.stats = {
            'total_rewards': [],
            'episode_rewards': 0.0,
            'episodes_completed': 0,
            'laps_completed': 0,
            'best_lap_reward': -float('inf'),
            'average_reward_last_100': 0.0,
            'exploration_rate': 0.0,
            'learning_progress': []
        }
        
        # Performance tracking
        self.episode_start_time = 0
        self.episode_steps = 0
        self.total_training_steps = 0

    def _get_difficulty_settings(self):
        """
        AI zorluk seviyelerine gÃ¶re parametreleri dÃ¶ndÃ¼r
        """
        difficulties = {
            "easy": {
                "learning_rate": 5e-5,      # YavaÅŸ Ã¶ÄŸrenme
                "memory_capacity": 10000,    # DÃ¼ÅŸÃ¼k hafÄ±za
                "eps_start": 0.95,          # Ã‡ok exploration
                "eps_end": 0.15,            # YÃ¼ksek minimum exploration
                "eps_decay": 3000,          # HÄ±zlÄ± decay
                "skill_modifier": 0.7       # DÃ¼ÅŸÃ¼k beceri
            },
            "medium": {
                "learning_rate": 1e-4,      # Normal Ã¶ÄŸrenme
                "memory_capacity": 20000,    # Normal hafÄ±za
                "eps_start": 0.9,           # Normal exploration
                "eps_end": 0.05,            # Normal minimum exploration
                "eps_decay": 5000,          # Normal decay
                "skill_modifier": 1.0       # Normal beceri
            },
            "hard": {
                "learning_rate": 2e-4,      # HÄ±zlÄ± Ã¶ÄŸrenme
                "memory_capacity": 30000,    # YÃ¼ksek hafÄ±za
                "eps_start": 0.8,           # Az exploration
                "eps_end": 0.02,            # DÃ¼ÅŸÃ¼k minimum exploration
                "eps_decay": 7000,          # YavaÅŸ decay
                "skill_modifier": 1.3       # YÃ¼ksek beceri
            },
            "expert": {
                "learning_rate": 3e-4,      # Ã‡ok hÄ±zlÄ± Ã¶ÄŸrenme
                "memory_capacity": 50000,    # Maksimum hafÄ±za
                "eps_start": 0.7,           # Minimum exploration
                "eps_end": 0.01,            # Ã‡ok dÃ¼ÅŸÃ¼k minimum exploration
                "eps_decay": 10000,         # Ã‡ok yavaÅŸ decay
                "skill_modifier": 1.5       # Maksimum beceri
            }
        }
        
        return difficulties.get(self.difficulty, difficulties["medium"])

    def _get_behavioral_params(self):
        """
        AI davranÄ±ÅŸ tipine gÃ¶re parametreleri dÃ¶ndÃ¼r
        """
        behaviors = {
            "aggressive": {
                "risk_tolerance": 0.8,      # YÃ¼ksek risk alma
                "speed_preference": 1.3,    # HÄ±z odaklÄ±
                "overtaking_tendency": 0.9, # Sollama eÄŸilimi
                "exploration_bonus": 1.2,   # Daha fazla keÅŸif
                "patience": 0.3             # DÃ¼ÅŸÃ¼k sabÄ±r
            },
            "balanced": {
                "risk_tolerance": 0.5,      # Orta risk
                "speed_preference": 1.0,    # Normal hÄ±z
                "overtaking_tendency": 0.5, # Normal sollama
                "exploration_bonus": 1.0,   # Normal keÅŸif
                "patience": 0.6             # Orta sabÄ±r
            },
            "defensive": {
                "risk_tolerance": 0.2,      # DÃ¼ÅŸÃ¼k risk
                "speed_preference": 0.8,    # GÃ¼venli hÄ±z
                "overtaking_tendency": 0.2, # Nadir sollama
                "exploration_bonus": 0.8,   # Az keÅŸif
                "patience": 0.9             # YÃ¼ksek sabÄ±r
            },
            "adaptive": {
                "risk_tolerance": 0.6,      # Duruma gÃ¶re deÄŸiÅŸken
                "speed_preference": 1.1,    # Adaptif hÄ±z
                "overtaking_tendency": 0.7, # Duruma gÃ¶re sollama
                "exploration_bonus": 1.1,   # Adaptif keÅŸif
                "patience": 0.5             # Orta sabÄ±r
            }
        }
        
        return behaviors.get(self.behavior_type, behaviors["balanced"])

    def share_experience(self, other_agents):
        """
        DiÄŸer ajanlarla deneyim paylaÅŸÄ±mÄ± yapar (Multi-Agent Learning)
        """
        if not self.competition_mode and len(other_agents) > 0:
            # En iyi performansa sahip ajanÄ± bul
            best_agent = max(other_agents, key=lambda agent: agent.stats['average_reward_last_100'])
            
            if best_agent.stats['average_reward_last_100'] > self.stats['average_reward_last_100']:
                # En iyi ajandan deneyim kopyala
                if len(best_agent.memory) > 100:
                    shared_transitions = random.sample(list(best_agent.memory.memory), 
                                                     min(50, len(best_agent.memory)))
                    
                    for transition in shared_transitions:
                        if random.random() < self.cooperation_factor:
                            self.memory.push(*transition)

    def update_rival_performance(self, other_agents):
        """
        Rakip ajanlarÄ±n performansÄ±nÄ± takip eder
        """
        for agent in other_agents:
            if agent.agent_id != self.agent_id:
                self.rival_performance[agent.agent_id] = {
                    'avg_reward': agent.stats['average_reward_last_100'],
                    'laps_completed': agent.stats['laps_completed'],
                    'behavior_type': agent.behavior_type,
                    'difficulty': agent.difficulty
                }

    def get_behavioral_action_modifier(self, state, base_action_probs):
        """
        DavranÄ±ÅŸ tipine gÃ¶re aksiyon seÃ§imini modifiye eder
        """
        behavior = self.behavioral_params
        
        # State'ten hÄ±z ve sensÃ¶r bilgilerini Ã§Ä±kar
        speed = state[0, 5].item() if len(state[0]) > 5 else 0.5
        min_sensor = min(state[0, :5]).item() if len(state[0]) >= 5 else 1.0
        
        # DavranÄ±ÅŸsal modifikasyonlar
        action_modifiers = torch.ones_like(base_action_probs)
        
        # Agresif davranÄ±ÅŸ: HÄ±zlÄ± gitmek iÃ§in ileri aksiyonunu artÄ±r
        if self.behavior_type == "aggressive":
            action_modifiers[0] *= (1.0 + behavior["speed_preference"] * 0.3)
            if min_sensor > 0.3:  # Yeterli alan varsa daha riskli
                action_modifiers[0] *= 1.2
        
        # Defansif davranÄ±ÅŸ: GÃ¼venli mesafe koruma
        elif self.behavior_type == "defensive":
            if min_sensor < 0.4:  # YakÄ±n engel varsa yavaÅŸla
                action_modifiers[0] *= 0.7
            # Daha fazla dÃ¶nÃ¼ÅŸ tercihi (gÃ¼venli geÃ§iÅŸ)
            action_modifiers[1] *= 1.1
            action_modifiers[2] *= 1.1
        
        # Adaptif davranÄ±ÅŸ: Duruma gÃ¶re deÄŸiÅŸken
        elif self.behavior_type == "adaptive":
            if min_sensor < 0.3:
                action_modifiers[1:] *= 1.3  # DÃ¶nÃ¼ÅŸleri artÄ±r
            elif speed < 0.7:
                action_modifiers[0] *= 1.4   # HÄ±zlanmayÄ± artÄ±r
        
        return action_modifiers

    def select_action(self, state, other_agents=None):
        """
        GeliÅŸmiÅŸ epsilon-greedy stratejisi + behavioral patterns
        """
        # DiÄŸer ajanlarÄ±n performansÄ±nÄ± gÃ¼ncelle
        if other_agents:
            self.update_rival_performance(other_agents)
        
        sample = random.random()
        # Epsilon deÄŸeri, zamanla azalÄ±r ve davranÄ±ÅŸ tipine gÃ¶re modifiye edilir
        base_eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \
            math.exp(-1. * self.steps_done / self.eps_decay)
        
        # DavranÄ±ÅŸ tipine gÃ¶re exploration modifikasyonu
        exploration_modifier = self.behavioral_params["exploration_bonus"]
        eps_threshold = base_eps_threshold * exploration_modifier
        
        self.steps_done += 1
        
        if sample > eps_threshold:
            # En iyi bilinen eylemi seÃ§ (exploitation) + behavioral modification
            with torch.no_grad():
                q_values = self.policy_net(state)
                
                # DavranÄ±ÅŸsal modifikasyon uygula
                behavioral_modifiers = self.get_behavioral_action_modifier(state, q_values[0])
                modified_q_values = q_values[0] * behavioral_modifiers
                
                # Skill modifier uygula (zorluk seviyesine gÃ¶re)
                if self.skill_modifier < 1.0:
                    # DÃ¼ÅŸÃ¼k skill - bazen yanlÄ±ÅŸ karar ver
                    if random.random() > self.skill_modifier:
                        return torch.tensor([[random.randrange(self.n_actions)]], 
                                           device=self.device, dtype=torch.long)
                
                return modified_q_values.max(0)[1].view(1, 1)
        else:
            # Behavioral pattern'e gÃ¶re exploration
            if self.behavior_type == "aggressive":
                # Agresif: Forward bias
                action_weights = [0.6, 0.2, 0.2]  # Forward, Left, Right
            elif self.behavior_type == "defensive":
                # Defansif: Turning bias
                action_weights = [0.3, 0.35, 0.35]  # Forward, Left, Right
            else:
                # Balanced/Adaptive: Uniform
                action_weights = [0.33, 0.33, 0.34]  # Forward, Left, Right
            
            # Weighted random selection
            action = random.choices(range(self.n_actions), weights=action_weights, k=1)[0]
            return torch.tensor([[action]], device=self.device, dtype=torch.long)

    def adaptive_difficulty_adjustment(self, performance_metrics):
        """
        Performansa gÃ¶re zorluk seviyesini otomatik ayarlar
        """
        avg_reward = performance_metrics.get('avg_reward', 0)
        win_rate = performance_metrics.get('win_rate', 0)
        
        # Performans Ã§ok iyi - zorluÄŸu artÄ±r
        if avg_reward > 80 and win_rate > 0.7:
            if self.difficulty != "expert":
                self.eps_end = max(0.01, self.eps_end * 0.8)
                self.skill_modifier = min(1.5, self.skill_modifier * 1.1)
                print(f"AI {self.agent_id} zorluk artÄ±rÄ±ldÄ±!")
        
        # Performans Ã§ok kÃ¶tÃ¼ - zorluÄŸu azalt
        elif avg_reward < 20 and win_rate < 0.2:
            if self.difficulty != "easy":
                self.eps_end = min(0.15, self.eps_end * 1.2)
                self.skill_modifier = max(0.7, self.skill_modifier * 0.9)
                print(f"AI {self.agent_id} zorluk azaltÄ±ldÄ±!")

    def optimize_model(self, batch_size=128, gamma=0.99):
        """
        HafÄ±zadan alÄ±nan bir grup deneyim Ã¼zerinden modeli eÄŸitir.
        """
        if len(self.memory) < batch_size:
            return # Yeterli deneyim birikmediyse Ã¶ÄŸrenme yapma

        transitions = self.memory.sample(batch_size)
        batch = Transition(*zip(*transitions))

        # Sonraki durumlarÄ±n bir maskesini oluÅŸtur (final olmayan durumlar iÃ§in)
        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                              batch.next_state)), device=self.device, dtype=torch.bool)
        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
        
        state_batch = torch.cat(batch.state)
        action_batch = torch.cat(batch.action)
        reward_batch = torch.cat(batch.reward)

        # Mevcut durumlar iÃ§in modelin tahmin ettiÄŸi Q-deÄŸerleri
        state_action_values = self.policy_net(state_batch).gather(1, action_batch)

        # Sonraki durumlar iÃ§in hedef Q-deÄŸerleri
        next_state_values = torch.zeros(batch_size, device=self.device)
        with torch.no_grad():
            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]
        
        # Beklenen Q-deÄŸerlerini hesapla (Bellman denklemi)
        expected_state_action_values = (next_state_values * gamma) + reward_batch

        # KayÄ±p (loss) fonksiyonunu hesapla
        criterion = nn.SmoothL1Loss()
        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

        # Geri yayÄ±lÄ±m ile modeli gÃ¼ncelle
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)
        self.optimizer.step()

    def update_target_net(self):
        """
        Hedef aÄŸÄ±, politika aÄŸÄ±nÄ±n aÄŸÄ±rlÄ±klarÄ±yla gÃ¼nceller.
        """
        self.target_net.load_state_dict(self.policy_net.state_dict())
    
    def update_stats(self, reward, lap_completed=False):
        """
        AI Ã¶ÄŸrenme istatistiklerini gÃ¼ncelle
        """
        self.stats['episode_rewards'] += reward
        self.episode_steps += 1
        self.total_training_steps += 1
        
        # Exploration rate gÃ¼ncelle
        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \
            math.exp(-1. * self.steps_done / self.eps_decay)
        self.stats['exploration_rate'] = eps_threshold
        
        if lap_completed:
            self.stats['laps_completed'] += 1
            
            # Episode tamamlandÄ±
            self.stats['episodes_completed'] += 1
            self.stats['total_rewards'].append(self.stats['episode_rewards'])
            
            # Best lap reward tracking
            if self.stats['episode_rewards'] > self.stats['best_lap_reward']:
                self.stats['best_lap_reward'] = self.stats['episode_rewards']
            
            # Average reward calculation (last 100 episodes)
            recent_rewards = self.stats['total_rewards'][-100:]
            self.stats['average_reward_last_100'] = sum(recent_rewards) / len(recent_rewards)
            
            # Learning progress tracking
            if len(self.stats['total_rewards']) % 10 == 0:  # Every 10 episodes
                progress_point = {
                    'episode': self.stats['episodes_completed'],
                    'avg_reward': self.stats['average_reward_last_100'],
                    'exploration_rate': eps_threshold,
                    'memory_size': len(self.memory)
                }
                self.stats['learning_progress'].append(progress_point)
            
            # Reset episode tracking
            self.stats['episode_rewards'] = 0.0
            self.episode_steps = 0
    
    def get_learning_stats(self):
        """
        AI Ã¶ÄŸrenme istatistiklerini dÃ¶ndÃ¼r
        """
        return {
            'episodes': self.stats['episodes_completed'],
            'laps': self.stats['laps_completed'],
            'avg_reward': self.stats['average_reward_last_100'],
            'best_reward': self.stats['best_lap_reward'],
            'exploration': self.stats['exploration_rate'],
            'memory_usage': len(self.memory),
            'total_steps': self.total_training_steps
        }
    
    def save_model(self, filepath):
        """
        AI modelini ve istatistiklerini kaydet
        """
        import os
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        save_data = {
            'policy_net_state': self.policy_net.state_dict(),
            'target_net_state': self.target_net.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'stats': self.stats,
            'steps_done': self.steps_done,
            'total_training_steps': self.total_training_steps,
            'episode_steps': self.episode_steps
        }
        
        torch.save(save_data, filepath)
        print(f"AI model saved to: {filepath}")
    
    def load_model(self, filepath):
        """
        AI modelini ve istatistiklerini yÃ¼kle
        """
        try:
            save_data = torch.load(filepath, map_location=self.device)
            
            self.policy_net.load_state_dict(save_data['policy_net_state'])
            self.target_net.load_state_dict(save_data['target_net_state'])
            self.optimizer.load_state_dict(save_data['optimizer_state'])
            self.stats = save_data['stats']
            self.steps_done = save_data['steps_done']
            self.total_training_steps = save_data['total_training_steps']
            self.episode_steps = save_data['episode_steps']
            
            print(f"AI model loaded from: {filepath}")
            print(f"Episodes: {self.stats['episodes_completed']}, Avg Reward: {self.stats['average_reward_last_100']:.2f}")
            return True
            
        except Exception as e:
            print(f"Error loading AI model: {e}")
            return False


class MultiAgentManager:
    """
    Birden fazla AI ajanÄ±nÄ± yÃ¶neten ve koordine eden sistem
    """
    def __init__(self):
        self.agents = []
        self.competition_results = []
        self.training_session_id = 0
        
        # Global learning settings
        self.global_learning_enabled = True
        self.experience_sharing_interval = 100  # Steps between sharing
        self.last_sharing_step = 0
        
        # Competition tracking
        self.race_winners = []
        self.performance_history = []
        
        # Behavioral diversity enforcement
        self.behavioral_balance = {
            "aggressive": 0,
            "balanced": 0, 
            "defensive": 0,
            "adaptive": 0
        }

    def add_agent(self, agent):
        """Sisteme yeni bir ajan ekler"""
        self.agents.append(agent)
        self.behavioral_balance[agent.behavior_type] += 1
        print(f"Agent {agent.agent_id} ({agent.behavior_type}) sisteme eklendi!")

    def remove_agent(self, agent_id):
        """Sistemden bir ajanÄ± Ã§Ä±karÄ±r"""
        for i, agent in enumerate(self.agents):
            if agent.agent_id == agent_id:
                self.behavioral_balance[agent.behavior_type] -= 1
                self.agents.pop(i)
                print(f"Agent {agent_id} sistemden Ã§Ä±karÄ±ldÄ±!")
                break

    def coordinate_learning(self):
        """Ajanlar arasÄ± koordineli Ã¶ÄŸrenme"""
        if not self.global_learning_enabled or len(self.agents) < 2:
            return
        
        current_step = sum(agent.total_training_steps for agent in self.agents)
        
        # Experience sharing periyodik olarak
        if current_step - self.last_sharing_step > self.experience_sharing_interval:
            self._facilitate_experience_sharing()
            self.last_sharing_step = current_step

    def _facilitate_experience_sharing(self):
        """Ajanlar arasÄ± deneyim paylaÅŸÄ±mÄ±nÄ± kolaylaÅŸtÄ±rÄ±r"""
        for agent in self.agents:
            other_agents = [a for a in self.agents if a.agent_id != agent.agent_id]
            agent.share_experience(other_agents)
            
        print("ğŸ¤ Multi-agent experience sharing completed!")

    def update_competition_results(self, race_results):
        """YarÄ±ÅŸ sonuÃ§larÄ±nÄ± gÃ¼nceller ve analiz eder"""
        self.competition_results.append(race_results)
        
        # Winner tracking
        if 'winner' in race_results:
            self.race_winners.append(race_results['winner'])
        
        # Performance analysis
        performance_data = {}
        for agent in self.agents:
            performance_data[agent.agent_id] = {
                'behavior': agent.behavior_type,
                'difficulty': agent.difficulty,
                'stats': agent.get_learning_stats(),
                'position': race_results.get(f'agent_{agent.agent_id}_position', 'unknown')
            }
        
        self.performance_history.append(performance_data)
        
        # Adaptive difficulty ayarlamasÄ±
        self._adjust_adaptive_difficulties()

    def _adjust_adaptive_difficulties(self):
        """Performansa gÃ¶re zorluk seviyelerini ayarlar"""
        if len(self.race_winners) < 10:  # En az 10 yarÄ±ÅŸ gerekli
            return
        
        recent_winners = self.race_winners[-10:]
        winner_performance = {}
        
        # Son 10 yarÄ±ÅŸÄ±n kazananlarÄ±nÄ± analiz et
        for winner_id in recent_winners:
            if winner_id not in winner_performance:
                winner_performance[winner_id] = 0
            winner_performance[winner_id] += 1
        
        # Ã‡ok baÅŸarÄ±lÄ± ajanlarÄ±n zorluÄŸunu artÄ±r
        for agent in self.agents:
            agent_wins = winner_performance.get(agent.agent_id, 0)
            win_rate = agent_wins / 10
            
            performance_metrics = {
                'avg_reward': agent.stats['average_reward_last_100'],
                'win_rate': win_rate
            }
            
            agent.adaptive_difficulty_adjustment(performance_metrics)

    def get_system_statistics(self):
        """Sistem geneli istatistikleri dÃ¶ndÃ¼rÃ¼r"""
        if not self.agents:
            return {"error": "No agents in system"}
        
        total_episodes = sum(agent.stats['episodes_completed'] for agent in self.agents)
        total_laps = sum(agent.stats['laps_completed'] for agent in self.agents)
        avg_system_reward = sum(agent.stats['average_reward_last_100'] for agent in self.agents) / len(self.agents)
        
        behavioral_performance = {}
        for behavior in self.behavioral_balance.keys():
            agents_of_type = [a for a in self.agents if a.behavior_type == behavior]
            if agents_of_type:
                avg_performance = sum(a.stats['average_reward_last_100'] for a in agents_of_type) / len(agents_of_type)
                behavioral_performance[behavior] = avg_performance
        
        return {
            'total_agents': len(self.agents),
            'total_episodes': total_episodes,
            'total_laps': total_laps,
            'average_system_reward': avg_system_reward,
            'behavioral_distribution': self.behavioral_balance.copy(),
            'behavioral_performance': behavioral_performance,
            'total_races': len(self.competition_results),
            'recent_winners': self.race_winners[-5:] if len(self.race_winners) >= 5 else self.race_winners,
            'training_session': self.training_session_id
        }

    def balance_behavioral_diversity(self):
        """DavranÄ±ÅŸsal Ã§eÅŸitliliÄŸi dengelemeye Ã§alÄ±ÅŸÄ±r"""
        total_agents = len(self.agents)
        if total_agents == 0:
            return
        
        # Ä°deal daÄŸÄ±lÄ±m: Her davranÄ±ÅŸ tipinden eÅŸit sayÄ±da
        ideal_per_type = total_agents // 4
        
        imbalanced_types = []
        for behavior, count in self.behavioral_balance.items():
            if abs(count - ideal_per_type) > 1:
                imbalanced_types.append((behavior, count, ideal_per_type))
        
        if imbalanced_types:
            print("âš–ï¸ Behavioral imbalance detected:")
            for behavior, current, ideal in imbalanced_types:
                print(f"  {behavior}: {current} (ideal: {ideal})")

    def start_training_session(self):
        """Yeni bir training session baÅŸlatÄ±r"""
        self.training_session_id += 1
        print(f"ğŸš€ Multi-agent training session #{self.training_session_id} started!")
        
        # Reset some statistics
        for agent in self.agents:
            agent.episode_steps = 0
        
        # Print session info
        stats = self.get_system_statistics()
        print(f"ğŸ“Š System: {stats['total_agents']} agents, Avg Reward: {stats['average_system_reward']:.2f}")
